{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waterhackweek 2020 Project - SnowGW: basin water budget components\n",
    "\n",
    "Based off of original code by [Emilio Mayorga](https://github.com/emiliom), University of Washington &mdash; [WaterHackWeek 2020](https://waterhackweek.github.io/). See Emilio's original script [mashup_waterbudget.ipynb](https://github.com/waterhackweek/waterdata/blob/master/mashup_waterbudget.ipynb) for helpful information on data sources, packages, and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "* Construct time series visualizations at daily and/or monthly timescales that show water budget components for basins with snow-groundwater dynamics of interest. (Interim)\n",
    "\n",
    "## Objective\n",
    "\n",
    "**Construct and analyze (sub)watershed-scale water budget components for the two example watersheds using geospatial watershed layers and a wide variety of water data, including:**\n",
    "\n",
    "* HUC8 basin boundaries (and associated HUC10 and HUC12 sub-basins)\n",
    "* Daymet precipitation and SWE (gridded, daily)\n",
    "* MODIS SSEBop evapotranspiration (gridded, monthly)\n",
    "* GRACE water storage anomalies (gridded, monthly)\n",
    "* USGS NWIS discharge (point, daily)\n",
    "* USGS groundwater level measurements (point, daily)\n",
    "* SNOTEL SWE (point, daily)\n",
    "\n",
    "## Cases\n",
    "\n",
    "### ONLY works for OR case right now!\n",
    "                                                  \n",
    "**Clackamas, OR**\n",
    "* HUC-8 Subbasin ID 17090011\n",
    "* size 2442 km^2\n",
    "* slope 24.2%\n",
    "* Total stream length (km) 1810.4 km\n",
    "* Land cover: evergreen forest (73%), shrub/scrub (10%)\n",
    "\n",
    "**Upper South Platte, CO**\n",
    "* HUC-8 Subbasin ID 10190002 \n",
    "* size: 4,797 kmÂ²\n",
    "* slope: 22.3%\n",
    "* Total stream length (km) 2812.4 km\n",
    "* Land cover: evergreen forest (50%), shrub/ scrub: 18%, grassland: 15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b style=\"font-size:120%\">Setup</b></br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import contextily as cx\n",
    "import folium\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import regionmask as rm\n",
    "\n",
    "from shapely.geometry import box\n",
    "import pynhd as nhd\n",
    "import pygeoogc as geoogc\n",
    "import pygeoutils as geoutils\n",
    "import hydrodata as hd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the time period\n",
    "\n",
    "To be reused with every dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = '2009-01-01'\n",
    "date_end = '2019-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select basin\n",
    "\n",
    "Select one of the case study basins' HUC8 codes (OR: `17090011`; CO: `10190002`)\n",
    "\n",
    "We'll use the shorthand \"sb\" to refer to the \"selected basin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sb_huc08code = '10190002'\n",
    "\n",
    "sb_huc08code = '17090011'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b style=\"font-size:120%\">Format basin</b></br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create HUC10 subwatersheds by first reading HUC12 subwatersheds\n",
    "\n",
    "Request all HUC12 watersheds whose HUC12 code includes the sb code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wfs_getfeatures_cqlfilter(wd, cql_filter=None):\n",
    "    \"\"\"\n",
    "    Use hydrodata packages to issue and process a OpenGeospatial Consortium (OGC) Web Feature Service (WFS) \n",
    "    request for WBD watersheds, with an optional filter to obtain only the watersheds we want.\n",
    "    Returns a nice and clean GeoPandas GeoDataframe in \"lat-lon\" projection (epsg:4326)\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"service\": \"wfs\",\n",
    "        \"request\": \"GetFeature\",\n",
    "        \"version\": wd.version,\n",
    "        \"outputFormat\": wd.outformat,\n",
    "        \"typeName\": wd.layer,\n",
    "    }\n",
    "    if type(cql_filter) is str:\n",
    "        payload[\"cql_filter\"] = cql_filter\n",
    "\n",
    "    r = geoogc.RetrySession().get(wd.url, payload)\n",
    "    \n",
    "    return geoutils.json2geodf(r.json(), \"epsg:4326\", crs=\"epsg:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_wdhuc12 = nhd.WaterData('huc12', crs='epsg:4269')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sb_wdhuc12.get_validnames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUC filter (cql_filter) will be the string \"huc12 LIKE '17030001%'\"\n",
    "sb_huc12_gdf = wfs_getfeatures_cqlfilter(\n",
    "    sb_wdhuc12, \n",
    "    cql_filter=f\"huc12 LIKE '{sb_huc08code}%'\"\n",
    ")\n",
    "\n",
    "sb_huc12_gdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `huc10` column from `huc12` codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_huc12_gdf['huc10'] = sb_huc12_gdf.huc12.str[:-2]\n",
    "cols = ['huc10', 'areasqkm', 'geometry']\n",
    "sb_huc10_gdf = sb_huc12_gdf[cols].dissolve(by='huc10', aggfunc='sum', as_index=False)\n",
    "\n",
    "len(sb_huc10_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_huc10_gdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `total_bounds` GeoDataFrame function to extract the outer bounding box coordinates, then create a simple GeoSeries out of that rectangle for reuse later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_huc10_gdf.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_bbox = gpd.GeoSeries(box(*sb_huc10_gdf.total_bounds), crs=sb_huc10_gdf.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `xarray` spatial (lat & lon) \"slices\" here, for reuse below when doing lat-lon region clipping (via the `.sel` method) of `xarray` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_lon_slice = slice(sb_bbox.bounds.minx[0], sb_bbox.bounds.maxx[0])\n",
    "sb_lat_slice = slice(sb_bbox.bounds.miny[0], sb_bbox.bounds.maxy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot HUC8 basin + HUC10 sub-basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: There are other basemap tiles besides StamenTerrain\n",
    "m = folium.Map(tiles='StamenTerrain', attr='ESRI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box\n",
    "folium.GeoJson(\n",
    "    sb_bbox.geometry,\n",
    "    style_function=lambda feature: {'color': 'red', 'fillOpacity': 0.0}\n",
    ").add_to(m)\n",
    "\n",
    "# Set the map extent (bounds) to the extent of the bounding box\n",
    "m.fit_bounds(m.get_bounds())\n",
    "\n",
    "# HUC10 sub-watershed polygons\n",
    "for i, i_gdf in sb_huc10_gdf.iterrows():\n",
    "    folium.GeoJson(\n",
    "        i_gdf.geometry,\n",
    "        style_function=lambda feature: {'color': 'black', 'fillOpacity': 0.0},\n",
    "        tooltip=i_gdf['huc10']\n",
    "    ).add_child(\n",
    "        folium.Popup(f\"<b>HUC10:</b><br> {i_gdf['huc10']}<br><b>watershed area:</b><br> {i_gdf['areasqkm']:.0f} km^2\")\n",
    "    ).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b style=\"font-size:120%\">Read data, and extract over selection (time and basin) </b></br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precipitation and SWE from DayMet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metadata (\"lazy loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_opendap_url = \"https://thredds.daac.ornl.gov/thredds-daymet/dodsC/daymet-v3-agg/na.ncml\"\n",
    "    \n",
    "daymet_ds = xr.open_dataset(daymet_opendap_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the selected basin bounding box in the DayMet projection coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_ds_grid_mapping_name = 'lambert_conformal_conic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DayMet crs information is in: ds.lambert_conformal_conic ('lcc')\n",
    "lcc_crs = CRS.from_cf(daymet_ds[daymet_ds_grid_mapping_name].attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject our bounding box polygon into this new coordinate system, to use in our request\n",
    "sb_bbox_lcc = sb_bbox.to_crs(lcc_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_lcc_bnd = sb_bbox_lcc.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the DayMet data for the selected XYT \"cube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_daymet_da = daymet_ds.sel(\n",
    "    time=slice(date_start, date_end),\n",
    "    x=slice(sb_lcc_bnd.minx[0], sb_lcc_bnd.maxx[0]),\n",
    "    y=slice(sb_lcc_bnd.maxy[0], sb_lcc_bnd.miny[0]) # note the reversed y values!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select precipitation and SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_daymet_da_prcp = sb_daymet_da.prcp\n",
    "sb_daymet_da_swe = sb_daymet_da.swe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproject to lat-lon (to use with other data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_da_as_ds(da, from_grid_mapping_name, from_crs, to_crs):\n",
    "    \"\"\"\n",
    "    Reproject an xarray DataArray using rioxarray and the expectation of CF convention \n",
    "    for projection information.\n",
    "    This function is fairly generic and can be reused with other datasets with only small changes.\n",
    "    Returns an xarray DataSet with the expected, CF-convention projection information.\n",
    "    \"\"\"\n",
    "    ds = da.to_dataset()\n",
    "    # Use rioxarray to improve (clean up) the CRS metadata, \n",
    "    # so it can be used in the reprojection step\n",
    "    ds.rio.write_crs(from_crs.to_string(), inplace=True)\n",
    "    \n",
    "    # Reproject the xarray dataset\n",
    "    # Note: the drop_vars step could be done outside this function, since it's source-data cleanup\n",
    "    reproj_ds = (\n",
    "        ds\n",
    "        .drop_vars(['lon', 'lat'])\n",
    "        .rio.reproject(to_crs)\n",
    "    )\n",
    "    \n",
    "    # Clean up the projection information and coordinates of the reprojected dataset\n",
    "    to_grid_mapping_name = reproj_ds[from_grid_mapping_name].attrs['grid_mapping_name']\n",
    "    reproj_ds = (\n",
    "        reproj_ds\n",
    "        .rio.set_spatial_dims('x', 'y')\n",
    "        .rio.write_crs(to_crs)\n",
    "        .rename({\"x\": \"lon\", \"y\": \"lat\", from_grid_mapping_name: to_grid_mapping_name}) \n",
    "        # this last step applies only when reprojecting to lat-lon\n",
    "    )\n",
    "    reproj_ds.attrs['grid_mapping'] = to_grid_mapping_name\n",
    "    reproj_ds[da.name].attrs['grid_mapping'] = to_grid_mapping_name\n",
    "    \n",
    "    return reproj_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_daymet_ds_prcp = reproject_da_as_ds(sb_daymet_da_prcp, daymet_ds_grid_mapping_name, lcc_crs, \"epsg:4326\")\n",
    "sb_daymet_ds_swe = reproject_da_as_ds(sb_daymet_da_swe, daymet_ds_grid_mapping_name, lcc_crs, \"epsg:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check on the resulting cell sizes, in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cellsizes(dimcoord):\n",
    "    print(dimcoord.values[1] - dimcoord.values[0], dimcoord.values[-1] - dimcoord.values[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cellsizes(sb_daymet_ds_prcp.lon)\n",
    "print_cellsizes(sb_daymet_ds_swe.lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cellsizes(sb_daymet_ds_prcp.lat)\n",
    "print_cellsizes(sb_daymet_ds_swe.lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip to basin polygon boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_daymet_ds_prcp = sb_daymet_ds_prcp.rio.clip(sb_huc10_gdf.geometry, all_touched=True)\n",
    "sb_daymet_ds_swe = sb_daymet_ds_swe.rio.clip(sb_huc10_gdf.geometry, all_touched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look (\"5th time step from the end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(7, 7))\n",
    "sb_daymet_ds_prcp.prcp.isel(time=-5).plot(ax=ax)\n",
    "sb_huc10_gdf.plot(ax=ax, edgecolor='white', facecolor='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(7, 7))\n",
    "sb_daymet_ds_swe.swe.isel(time=-5).plot(ax=ax)\n",
    "sb_huc10_gdf.plot(ax=ax, edgecolor='white', facecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETa from MODIS SSEBop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssebop_opendap_url = \"https://cida.usgs.gov/thredds/dodsC/ssebopeta/monthly\"\n",
    "\n",
    "ssebop_ds = xr.open_dataset(ssebop_opendap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssebop_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssebop_ds.rio.write_crs(CRS.from_cf(ssebop_ds['crs'].attrs), inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssebop_ds.time.values[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_ssebop_et_ds = ssebop_ds.sel(time=slice(date_start, date_end), lon=sb_lon_slice, lat=sb_lat_slice)\n",
    "\n",
    "sb_ssebop_et_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_ssebop_et_ds = sb_ssebop_et_ds.rio.clip(sb_huc10_gdf.geometry, all_touched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(7, 7))\n",
    "sb_ssebop_et_ds.et.isel(time=6).plot(ax=ax)\n",
    "sb_huc10_gdf.plot(ax=ax, edgecolor='white', facecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWS anomalies from GRACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_opendap_url = \"https://podaac-opendap.jpl.nasa.gov/opendap/allData/tellus/L3/mascon/RL06/JPL/v02/CRI/netcdf/GRCTellus.JPL.200204_202007.GLO.RL06M.MSCNv02CRI.nc\"\n",
    "\n",
    "grace_ds = xr.open_dataset(grace_opendap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_ds.lon.values.min(), grace_ds.lon.values.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** `lon` values are 0 - 360 (starting from the anti-meridian, \"-180\") rather than -180 to 180 as the previous datasets; add an offset in the lon slice specification, and shift the xarray dataset to make it work with the region mask and GeoDataFrame overlays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_grace_lwe_thickness_ds = grace_ds.sel(\n",
    "    time=slice(date_start, date_end), \n",
    "    lon=slice(360 + sb_bbox.bounds.minx[0] - 0.5, 360 + sb_bbox.bounds.maxx[0] + 0.5), \n",
    "    lat=slice(sb_bbox.bounds.miny[0] - 0.5, sb_bbox.bounds.maxy[0] + 0.5), \n",
    ")\n",
    "\n",
    "sb_grace_lwe_thickness_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_grace_lwe_thickness_ds.coords['lon'] = (sb_grace_lwe_thickness_ds.coords['lon'] + 180) % 360 - 180\n",
    "sb_grace_lwe_thickness_ds = sb_grace_lwe_thickness_ds.sortby(sb_grace_lwe_thickness_ds.lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_grace_lwe_thickness_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(7, 7))\n",
    "sb_grace_lwe_thickness_ds.lwe_thickness.isel(time=6).plot(ax=ax)\n",
    "sb_huc10_gdf.plot(ax=ax, edgecolor='white', facecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discharge from USGS NWIS\n",
    "\n",
    "Use `hydrodata.interactive_map`: for HUC10 subbasin boundaries, produce clickable site markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = hd.interactive_map(tuple(sb_bbox.total_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, i_gdf in sb_huc10_gdf.iterrows():\n",
    "    folium.GeoJson(\n",
    "        i_gdf.geometry,\n",
    "        style_function=lambda feature: {'color': 'black', 'fillOpacity': 0.0},\n",
    "        tooltip=i_gdf['huc10']\n",
    "    ).add_child(\n",
    "        folium.Popup(f\"<b>HUC10:</b><br> {i_gdf['huc10']}<br><b>watershed area:</b><br> {i_gdf['areasqkm']:.0f} km^2\")\n",
    "    ).add_to(m2)\n",
    "\n",
    "\n",
    "m2.fit_bounds(m2.get_bounds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"font-size:100%\">\n",
    "\n",
    "<b>HELP?</b> would be great to have some script that allowed us to subset guages to those within the above-selected date range!\n",
    "    \n",
    "Right now, I'm just clicking through the pins to find a site with an appropriate date range, and then entering that site number below (`nearestoutlet_sitecode = [manually selected site]`)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis = hd.NWIS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearestoutlet_sitecode = '14211010'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `mmd=True`, `nwis.get_streamflow` converts cms (cubic meters / second) to mmd (mm/day) based on the contributing drainage area of the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_q_daily_ts = nwis.get_streamflow([nearestoutlet_sitecode], dates=(date_start, date_end), mmd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_q_daily_ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample to daily and tweak it so it's consistent with the other watershed-scale time series. \n",
    "\n",
    "NOTE: how to input nearestoutlet_sitecode as a variable below (instead of column site name directly)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_q_ts_df = nwis_q_daily_ts.resample('1M').sum()\n",
    "nwis_q_ts_df.index = nwis_q_ts_df.index.month\n",
    "nwis_q_ts_df.index.name = 'month'\n",
    "nwis_q_ts_df.rename(columns={'USGS-14211010': 'disch'}, inplace=True)\n",
    "\n",
    "nwis_q_ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundwater level from USGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw = pd.read_csv(\"data/ORgw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw.rename(columns={'116034_72019_00003': 'gw'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw['datetime'] =  pd.to_datetime(sb_gw['datetime'], format='%Y-%m-%d')\n",
    "sb_gw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw=sb_gw.set_index(sb_gw.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw=sb_gw.loc[slice(date_start,date_end)]\n",
    "sb_gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate to monthly (mean) and label units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_gw_ts_df = sb_gw.resample('1M').mean()\n",
    "sb_gw_ts_df.index = sb_gw_ts_df.index.month\n",
    "sb_gw_ts_df.index.name = 'month'\n",
    "sb_gw_ts_df = sb_gw_ts_df[['gw']]\n",
    "\n",
    "sb_gw_ts_df.attrs['long_name'] = 'monthly mean water level - depth below surface'\n",
    "sb_gw_ts_df.attrs['units'] = 'feet'\n",
    "\n",
    "sb_gw_ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWE from SNOTEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stn = pd.read_csv(\"data/OR_DB.csv\",dtype={'huc8':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn=snow_stn[snow_stn.huc8 == sb_huc08code]\n",
    "stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ulmo\n",
    "wsdlurl = \"http://hydroportal.cuahsi.org/Snotel/cuahsi_1_1.asmx?WSDL\"\n",
    "\n",
    "def fetch(sitecode, variablecode, start_date, end_date):\n",
    "    print(sitecode, variablecode, start_date, end_date)\n",
    "    values_df = None\n",
    "    try:\n",
    "        #Request data from the server\n",
    "        site_values = ulmo.cuahsi.wof.get_values(\n",
    "            wsdlurl, 'SNOTEL:'+sitecode, variablecode, start=start_date, end=end_date\n",
    "        )\n",
    "        #Convert to a Pandas DataFrame   \n",
    "        values_df = pd.DataFrame.from_dict(site_values['values'])\n",
    "        #Parse the datetime values to Pandas Timestamp objects\n",
    "        values_df['datetime'] = pd.to_datetime(values_df['datetime'])\n",
    "        #Set the DataFrame index to the Timestamps\n",
    "        values_df.set_index('datetime', inplace=True)\n",
    "        #Convert values to float and replace -9999 nodata values with NaN\n",
    "        values_df['value'] = pd.to_numeric(values_df['value']).replace(-9999, np.nan)\n",
    "        #Remove any records flagged with lower quality\n",
    "        values_df = values_df[values_df['quality_control_level_code'] == '1']\n",
    "    except:\n",
    "        print(\"Unable to fetch %s\" % variablecode)\n",
    "    \n",
    "    return values_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_swe = pd.DataFrame(index=pd.date_range(start=date_start, end=date_end))\n",
    "for sitecode in stn.code:\n",
    "    try:\n",
    "        swe = fetch(sitecode, variablecode='SNOTEL:WTEQ_D', start_date=date_start, end_date=date_end)\n",
    "        stn_swe[sitecode] = swe.value\n",
    "    except:\n",
    "        print(sitecode, 'has no data')\n",
    "        stn_swe[sitecode] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_swe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate to monthly (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_swe_ts_df = stn_swe.resample('1M').mean()\n",
    "stn_swe_ts_df.index = stn_swe_ts_df.index.month\n",
    "stn_swe_ts_df.index.name = 'month'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset to a single site (either one) and label units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stn_swe_ts_df.rename(columns={'398_OR_SNTL': 'stn_swe'}, inplace=True) # just renaming one\n",
    "stn_swe_ts_df = stn_swe_ts_df[['stn_swe']]\n",
    "\n",
    "stn_swe_ts_df.attrs['long_name'] = 'monthly mean SNOTEL SWE'\n",
    "stn_swe_ts_df.attrs['units'] = 'mm/month'\n",
    "stn_swe_ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b style=\"font-size:120%\">Generate basin time series means using basin regions and mask (HUC10-level) </b></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `regionmask` regions from a GeoDataFrame\n",
    "\n",
    "<b>REGIONMASK LIMITATION:</b> <a href=\"https://github.com/mathause/regionmask/issues/151\">regionmask is hard-wired to work only with data in lat-lon coordinates!</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_huc10_regions = rm.from_geopandas(\n",
    "    sb_huc10_gdf, names=\"huc10\", name=\"huc10\"\n",
    ")\n",
    "\n",
    "sb_huc10_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_huc10_regions.plot(label=\"name\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUC10 watershed time series means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_huc10_ts(regions, ds, dsvarname, crsvar=None):\n",
    "    \"\"\"\n",
    "    Use regionmask to create a gridded mask from the regions and the xarray dataset's\n",
    "    grid resolution. Copy the variable attributes from the source DataSet to the \n",
    "    output DataArray.\n",
    "    Returns a DataArray\n",
    "    \"\"\"\n",
    "    mask3d = regions.mask_3D(ds, drop=False)\n",
    "\n",
    "    # Latitude weighting for area-weighted averaging.\n",
    "    # But it's probably unnecessary for this small geographical extent of the Upper Yakima\n",
    "    weights = np.cos(np.deg2rad(ds.lat))\n",
    "    regions_ts = ds[dsvarname].weighted(mask3d * weights).mean(dim=(\"lat\", \"lon\"))\n",
    "    # Assign dataarray name, and variable long name and units to region_ts, from ds[dsvarname]\n",
    "    regions_ts.name = dsvarname\n",
    "    regions_ts.attrs['long_name'] = ds[dsvarname].attrs['long_name']\n",
    "    regions_ts.attrs['units'] = ds[dsvarname].attrs['units']\n",
    "    \n",
    "    # if present, drop crs variable b/c it's not needed anymore\n",
    "    if crsvar in ds:\n",
    "        regions_ts = regions_ts.drop_vars(crsvar)\n",
    "    \n",
    "    return regions_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precipitation and SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_prcp_regions_ts_daily = generate_huc10_ts(sb_huc10_regions, sb_daymet_ds_prcp, 'prcp', crsvar='latitude_longitude')\n",
    "daymet_swe_regions_ts_daily = generate_huc10_ts(sb_huc10_regions, sb_daymet_ds_swe, 'swe', crsvar='latitude_longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate (sum) to monthly resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_prcp_regions_ts = daymet_prcp_regions_ts_daily.resample(time='1M').sum()\n",
    "daymet_prcp_regions_ts.attrs['long_name'] = 'monthly total precipitation'\n",
    "daymet_prcp_regions_ts.attrs['units'] = 'mm/month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_swe_regions_ts = daymet_swe_regions_ts_daily.resample(time='1M').mean()\n",
    "daymet_swe_regions_ts.attrs['long_name'] = 'monthly mean SWE'\n",
    "daymet_swe_regions_ts.attrs['units'] = 'mm/month' # km/m2 == mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evapotranspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssebop_et_regions_ts = generate_huc10_ts(sb_huc10_regions, sb_ssebop_et_ds, 'et', crsvar='crs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_lwe_thickness_regions_ts = generate_huc10_ts(sb_huc10_regions, sb_grace_lwe_thickness_ds, 'lwe_thickness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert units to mm, and update units attribute accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_lwe_thickness_regions_ts *= 10\n",
    "grace_lwe_thickness_regions_ts.attrs['units'] = 'mm/month'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot spatial data summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrange = 600 # in mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(ncols=4,figsize=(20, 4))\n",
    "daymet_prcp_regions_ts.plot(ax=ax[0], hue=\"region\")\n",
    "ax[0].set_title('Precipitation')\n",
    "ax[0].set_ylim(0, yrange)\n",
    "daymet_swe_regions_ts.plot(ax=ax[1], hue=\"region\")\n",
    "ax[1].set_title('Snow Water Equivalent')\n",
    "ax[1].set_ylim(0, yrange-500)\n",
    "ssebop_et_regions_ts.plot(ax=ax[2], hue=\"region\")\n",
    "ax[2].set_title('Evapotranspiration')\n",
    "ax[2].set_ylim(0, yrange)\n",
    "grace_lwe_thickness_regions_ts.plot(ax=ax[3], hue=\"region\")\n",
    "ax[3].set_title('Water Storage')\n",
    "ax[3].set_ylim(-200, -200 + yrange);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole-watershed DataFrames\n",
    "\n",
    "Generate sb-wide PPT, SWE, ET and LWE_thickness time series from HUC10-level values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_huc10_gdf['areaweight'] = sb_huc10_gdf.areasqkm.values / sb_huc10_gdf.areasqkm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sb_ts(da, davarname):\n",
    "    \"\"\"\n",
    "    Calculate whole-watershed time series based on area-weighted averaging of\n",
    "    HUC10-level time series, converting from DataArray to DataFrame.\n",
    "    Returns a cleaned and consistent DataFrame\n",
    "    \"\"\"\n",
    "    areawt = xr.DataArray(\n",
    "        sb_huc10_gdf.areasqkm.values / sb_huc10_gdf.areasqkm.sum(), \n",
    "        dims=('region'), \n",
    "        coords={'region': sb_huc10_gdf.index}\n",
    "    )\n",
    "    sb_ts = da.weighted(areawt).mean(dim=(\"region\"))\n",
    "    # Assign dataarray name\n",
    "    sb_ts.name = davarname\n",
    "    sb_ts_df = sb_ts.to_dataframe()\n",
    "    sb_ts_df.index = sb_ts_df.index.month\n",
    "    sb_ts_df.index.name = 'month'\n",
    "    \n",
    "    return sb_ts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precipitation and SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_prcp_sb_ts_df = generate_sb_ts(daymet_prcp_regions_ts, 'prcp')\n",
    "daymet_swe_sb_ts_df = generate_sb_ts(daymet_swe_regions_ts, 'swe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evapotranspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssebop_et_sb_ts_df = generate_sb_ts(ssebop_et_regions_ts, 'et')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_lwe_thickness_sb_tsdf = generate_sb_ts(grace_lwe_thickness_regions_ts, 'lwe_thickness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add in well level, snotel, discharge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"font-size:100%\">\n",
    "<b style=\"font-size:120%\">All-variable data frame </b></br>\n",
    "\n",
    "HUC8 basin and monthly time resolution for precip, ET, discharge, SWE (Daymet), SWE (**single SNOTEL site in basin**), GW level (**single USGS site in basin**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_df = pd.concat([daymet_prcp_sb_ts_df, ssebop_et_sb_ts_df, nwis_q_ts_df, daymet_swe_sb_ts_df, stn_swe_ts_df, sb_gw_ts_df], \n",
    "                              axis='columns')\n",
    "monthly_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:whw]",
   "language": "python",
   "name": "conda-env-whw-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
